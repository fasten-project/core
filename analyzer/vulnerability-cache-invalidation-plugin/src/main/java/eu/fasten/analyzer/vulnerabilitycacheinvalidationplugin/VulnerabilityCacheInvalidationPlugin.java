/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package eu.fasten.analyzer.vulnerabilitycacheinvalidationplugin;

import eu.fasten.core.data.Constants;
import eu.fasten.core.maven.GraphMavenResolver;
import eu.fasten.core.maven.data.Revision;
import eu.fasten.core.maven.utils.MavenUtilities;
import eu.fasten.core.plugins.DataWriter;
import eu.fasten.core.plugins.DependencyGraphUser;
import eu.fasten.core.plugins.KafkaPlugin;
import it.unimi.dsi.fastutil.objects.ObjectLinkedOpenHashSet;
import org.jooq.DSLContext;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;
import org.pf4j.Extension;
import org.pf4j.Plugin;
import org.pf4j.PluginWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import java.io.File;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.util.Collections;
import java.util.List;
import java.util.Optional;

public class VulnerabilityCacheInvalidationPlugin extends Plugin {
    public VulnerabilityCacheInvalidationPlugin(PluginWrapper wrapper) {
        super(wrapper);
    }

    @Extension
    public static class VulnerabilityCacheInvalidationExtension implements KafkaPlugin, DependencyGraphUser, DataWriter {

        private final Logger logger = LoggerFactory.getLogger(VulnerabilityCacheInvalidationExtension.class.getName());

        private String consumerTopic = "fasten.GraphDBExtension.out";

        private static GraphMavenResolver graphResolver;
        private static String baseDir;
        private Exception pluginError = null;
        private ObjectLinkedOpenHashSet<Revision> depSet;

        /**
         * The helper method that creates a graph resolver.
         * It first creates a Database Context from Knowledge Base and
         * then uses it to build dependency graph in the graph resolver.
         *
         * @param dbContext    - Connection to the database
         * @param depGraphPath - the directory where the dependency graph can be found.
         */
        public void loadGraphResolver(DSLContext dbContext, String depGraphPath) {
            logger.info("Building Dependency Graph from " + depGraphPath + "...");
            try {
                var graphResolver = new GraphMavenResolver();
                graphResolver.buildDependencyGraph(dbContext, depGraphPath);
                VulnerabilityCacheInvalidationExtension.graphResolver = graphResolver;
            } catch (SQLException e) {
                var err = "Couldn't connect to the KnowledgeBase";
                logger.error(err, e);
                this.setPluginError(new SQLException(err, e));
            } catch (Exception e) {
                var err = "Couldn't build the dependency graph";
                logger.error(err, e);
                this.setPluginError(new RuntimeException(err, e));
            }
            logger.info("...Dependency Graph has been successfully built.");
        }

        /**
         * The helper method that creates a graph resolver.
         * It is overloaded method that allows to load graph resolver from the mocked instance.
         * Currently, used for testing purposes.
         *
         * @param mockResolver - mocked instance of GraphMavenResolver.
         */
        public void loadGraphResolver(GraphMavenResolver mockResolver) {
            logger.info("Loaded mock graph resolver");
            VulnerabilityCacheInvalidationExtension.graphResolver = mockResolver;
        }

        @Override
        public void setBaseDir(String baseDir) {
            VulnerabilityCacheInvalidationExtension.baseDir = baseDir;
        }

        @Override
        public String name() {
            return "Vulnerability Cache Invalidation Plugin";
        }

        @Override
        public String description() {
            return "Vulnerability Cache Invalidation Plugin. "
                    + "Consumes list of updated product from Kafka"
                    + " topic and invalidates cache of vulnerable paths"
                    + " for all its transitive dependants.";
        }

        @Override
        public String version() {
            return "0.0.1";
        }

        @Override
        public void start() {
        }

        @Override
        public void stop() {
            VulnerabilityCacheInvalidationExtension.graphResolver = null;
        }

        public void setPluginError(Exception throwable) {
            this.pluginError = throwable;
        }

        @Override
        public Exception getPluginError() {
            return this.pluginError;
        }

        @Override
        public void freeResource() {
        }

        @Override
        public Optional<List<String>> consumeTopic() {
            return Optional.of(Collections.singletonList(consumerTopic));
        }

        @Override
        public void setTopic(String topicName) {
            this.consumerTopic = topicName;
        }

        @Override
        public void consume(String record) {
            this.depSet = null;
            this.setPluginError(null);

            // Parse JSON object from kafka topic of GraphDBExtension.
            // Although it doesn't have output payload, the plugin serializes the graph for its input.
            // And we can use the input copy from this topic and the serialized graph to process our caching.
            var json = new JSONObject(record);

            if (json.has("input")) {
                if (json.get("input").toString().isEmpty()) {
                    logger.error("Empty input");
                    setPluginError(new RuntimeException("Empty input"));
                    return;
                }
                json = json.getJSONObject("input").getJSONObject("payload");
            }

            // Parse input values for the root product.
            String groupId = "";
            String artifactId = "";
            String version = "";
            try {
                version = json.getString("version");
                var product = json.getString("product");
                var splits = product.split(Constants.mvnCoordinateSeparator);
                groupId = splits[0];
                artifactId = splits[1];
            } catch (JSONException e) {
                logger.error("Error parsing product for vulnerability cache invalidator", e);
                setPluginError(e);
                return;
            }
            
            // Resolve the set of transitive dependants for this product.
            this.depSet = VulnerabilityCacheInvalidationExtension.graphResolver.resolveDependents(groupId, artifactId, version, -1, true);

            this.depSet.add(new Revision(groupId, artifactId, version, new Timestamp(-1)));

            // Go over the set and invalidate the cache for each dependant.
            for (Revision revision : depSet) {
                var firstLetter = revision.groupId.substring(0, 1);
                var outputPath = baseDir + File.separator + firstLetter +
                        File.separator + revision.groupId +
                        File.separator + revision.artifactId +
                        File.separator + revision.version.toString() + ".json";

                File outputFile = new File(outputPath);
                if (!outputFile.exists() || outputFile.length() == 0) continue;

                // TODO: possible optimization: we don't really need to delete the whole file, or do we?
                MavenUtilities.forceDeleteFile(outputFile);
            }
        }

        @Override
        public Optional<String> produce() {
            if (this.depSet == null) {
                return Optional.empty();
            } else {
                var jsonDepSet = new JSONArray();
                // TODO: possible optimization: we don't really need to pass the whole revision.
                depSet.stream().map(Revision::toJSON).forEach(jsonDepSet::put);
                var json = new JSONObject();
                json.put("artifacts", jsonDepSet);
                return Optional.of(json.toString());
            }
        }

        @Override
        public String getOutputPath() {
            return null;
        }
    }

}

