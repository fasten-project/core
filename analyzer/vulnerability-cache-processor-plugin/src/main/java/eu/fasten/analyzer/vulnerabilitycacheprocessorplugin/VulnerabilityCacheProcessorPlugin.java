/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package eu.fasten.analyzer.vulnerabilitycacheprocessorplugin;

import eu.fasten.core.data.DirectedGraph;
import eu.fasten.core.data.callableindex.RocksDao;
import eu.fasten.core.data.metadatadb.MetadataDao;
import eu.fasten.core.maven.GraphMavenResolver;
import eu.fasten.core.maven.data.Revision;
import eu.fasten.core.merge.CGMerger;
import eu.fasten.core.plugins.DataWriter;
import eu.fasten.core.plugins.DependencyGraphUser;
import eu.fasten.core.plugins.CallableIndexReader;
import eu.fasten.core.plugins.KafkaPlugin;
import eu.fasten.core.utils.FastenUriUtils;
import org.jooq.DSLContext;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;
import org.pf4j.Extension;
import org.pf4j.Plugin;
import org.pf4j.PluginWrapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.stream.Collectors;

public class VulnerabilityCacheProcessorPlugin extends Plugin {
    public VulnerabilityCacheProcessorPlugin(PluginWrapper wrapper) {
        super(wrapper);
    }

    @Extension
    public static class VulnerabilityCacheProcessorExtension implements KafkaPlugin, DependencyGraphUser, DataWriter,
        CallableIndexReader {

        private final Logger logger = LoggerFactory.getLogger(VulnerabilityCacheProcessorExtension.class.getName());
        private String consumerTopic = "fasten.VulnerabilityCacheInvalidationExtension.out";

        private Exception pluginError = null;

        private static GraphMavenResolver graphResolver;
        private static String baseDir;

        private static DSLContext dbContext;
        private static MetadataDao kbDao;
        private static RocksDao graphDao;

        /**
         * The helper method that creates a graph resolver.
         * It first creates a Database Context from Knowledge Base and
         * then uses it to build dependency graph in the graph resolver.
         *
         * @param dbContext    - Connection to the database.
         * @param depGraphPath - the directory where the dependency graph can be found.
         */
        @Override
        public void loadGraphResolver(DSLContext dbContext, String depGraphPath) {
            logger.info("Building Dependency Graph from " + depGraphPath + "...");
            try {
                VulnerabilityCacheProcessorExtension.dbContext = dbContext;
                VulnerabilityCacheProcessorExtension.kbDao = new MetadataDao(dbContext);
                var graphResolver = new GraphMavenResolver();
                graphResolver.buildDependencyGraph(dbContext, depGraphPath);
                VulnerabilityCacheProcessorExtension.graphResolver = graphResolver;
            } catch (SQLException e) {
                var err = "Couldn't connect to the Knowledge Base";
                logger.error(err, e);
                this.setPluginError(new SQLException(err, e));
            } catch (RuntimeException e) {
                var err = "Couldn't connect to the Graph Database";
                logger.error(err, e);
                this.setPluginError(new RuntimeException(err, e));
            } catch (Exception e) {
                var err = "Couldn't build the Dependency Graph";
                logger.error(err, e);
                this.setPluginError(new RuntimeException(err, e));
            }
            logger.info("...Dependency Graph has been successfully built.");
        }

        /**
         * The helper method that creates a graph resolver.
         * It is overloaded method that allows to load graph resolver from the mocked instance.
         * Currently, used for testing purposes.
         *
         * @param dbContext      - Connection to the database.
         * @param kbDao          - Knowledge Base Dao.
         * @param graphDao       - Graph DB Dao.
         * @param graphResolver  - Graph Resolver.
         */
        public void loadGraphResolver(DSLContext dbContext, MetadataDao kbDao, RocksDao graphDao, GraphMavenResolver graphResolver) {
            VulnerabilityCacheProcessorExtension.dbContext = dbContext;
            VulnerabilityCacheProcessorExtension.kbDao = kbDao;
            VulnerabilityCacheProcessorExtension.graphDao = graphDao;
            VulnerabilityCacheProcessorExtension.graphResolver = graphResolver;
        }

        /**
         * Helper method that traverses the directed graph and creates paths from source to vulnerable dependency.
         *
         * @param graph           - directed graph of the dependencies.
         * @param source          - source artifact.
         * @param target          - target vulnerable dependency.
         * @param visited         - helper argument for graph traverse.
         * @param path            - helper argument for preliminary path.
         * @return the list of paths for each vulnerable dependency.
         */
        private List<Long> getPathsToVulnerableNode(DirectedGraph graph, long source, long target,
                                                          Set<Long> visited, List<Long> path) {
            path.add(source);
            if (source == target) {
                return path;
            }
            visited.add(source);
            for (var node : graph.successors(source)) {
                if (!visited.contains(node)) {
//                    path.add(node);
                    getPathsToVulnerableNode(graph, node, target, visited, path);
//                    path.remove(node);
                }
            }
            visited.remove(source);
            return path;
        }

        /**
         * Helper method to persist calculated paths into a file.
         * @param revision - the revision that was processed.
         * @param output   - the output body with vulnerable paths.
         */
        private void persistOutput(Revision revision, String output) {
            try {
                var firstLetter = revision.groupId.substring(0, 1);
                var outputPath = baseDir + File.separator + firstLetter +
                        File.separator + revision.groupId +
                        File.separator + revision.artifactId +
                        File.separator + revision.version.toString() + ".json";

                File outputFile = new File(outputPath);
                outputFile.setExecutable(true, false);
                outputFile.setWritable(true, false);
                outputFile.setReadable(true, false);

                if(!outputFile.getParentFile().exists()) {
                    logger.info("Creating directory for the cache file: " + outputFile.getParentFile().toString());
                    var dir = outputFile.getParentFile().mkdir();
                    logger.info("Result: " + dir);
                }
                if(!outputFile.exists()) outputFile.createNewFile();

                FileWriter writer = new FileWriter(outputFile);
                writer.write(output);
                writer.close();
                logger.info("Persisted cache file for revision: " + revision.toString());
            } catch (IOException e) {
                logger.error("Unable to persist the cache output in the file", e);
                setPluginError(e);
            }
        }

        @Override
        public void setRocksDao(RocksDao rocksDao) {
            VulnerabilityCacheProcessorExtension.graphDao = rocksDao;
        }

        @Override
        public void setBaseDir(String baseDir) {
            VulnerabilityCacheProcessorExtension.baseDir = baseDir;
        }

        @Override
        public String name() {
            return "Vulnerability Cache Processor Plugin";
        }

        @Override
        public String description() {
            return "Vulnerability Cache Processor Plugin. "
                    + "Consumes list MergeCacheInvalidationPlugin from Kafka"
                    + " topic and caches vulnerable paths to a file";
        }

        @Override
        public String version() {
            return "0.0.1";
        }

        @Override
        public void start() {
        }

        @Override
        public void stop() {
            graphDao.close();
            graphDao = null;
        }

        public void setPluginError(Exception throwable) {
            this.pluginError = throwable;
        }

        @Override
        public Exception getPluginError() {
            return this.pluginError;
        }

        @Override
        public void freeResource() {
            graphDao.close();
            graphDao = null;
        }

        @Override
        public Optional<List<String>> consumeTopic() {
            return Optional.of(Collections.singletonList(consumerTopic));
        }

        @Override
        public void setTopic(String topicName) {
            this.consumerTopic = topicName;
        }

        @Override
        public void consume(String record) {
            this.pluginError = null;

            // Parse JSON object from kafka topic of GraphDBExtension.
            // Although it doesn't have output payload, the plugin serializes the graph for its input.
            // And we can use the input copy from this topic and the serialized graph to process our caching.
            var json = new JSONObject(record);
            if (json.has("payload")) {
                if (json.get("payload").toString().isEmpty()) {
                    logger.error("Empty payload");
                    setPluginError(new RuntimeException("Empty payload"));
                    return;
                }
                json = json.getJSONObject("payload");
            } else {
                logger.error("Payload is absent");
                setPluginError(new RuntimeException("Payload is absent"));
                return;
            }

            try {
                var artifacts = json.getJSONArray("artifacts");

                for (int i = 0; i < json.length(); i++) {
                    var revisionObj = artifacts.getJSONObject(i);
                    var id = revisionObj.optLong("id", -1);
                    var groupId = revisionObj.getString("groupId");
                    var artifactId = revisionObj.getString("artifactId");
                    var version = revisionObj.getString("version");
                    var timestamp = revisionObj.optLong("createdAt", -1);
                    var revision = new Revision(id, groupId, artifactId, version, new Timestamp(timestamp));
                    logger.info("Processing revision: " + revision.toString());

                    // Sometimes the topic passes in artifacts with invalid revision id of 0.
                    if (revision.id == 0) {
                        logger.info("Failed to retrieve correct revision id from the topic. Trying to fetch it from DB...");
                        var fixedId = kbDao.getPackageVersionID(revision.product().toString(), revision.version.toString());
                        if(fixedId > 0) {
                            logger.info("Revision ID is fixed. Continuing the process...");
                            revision.id = fixedId;
                        } else {
                            logger.info("Failed to retrieve correct revision id from DB. Skipping the process...");
                            persistOutput(revision, "{}");
                            continue;
                        }
                    }

                    var depSet = graphResolver.resolveDependencies(revision, dbContext, true);
                    var depIds = depSet.stream().map(r -> r.id).collect(Collectors.toSet());
                    logger.info("Revision ID: " + revision.id);
                    logger.info("Dep IDs: " + depIds.toString());

                    var vulnerableDependencies = kbDao.findVulnerablePackageVersions(depIds);
                    if (vulnerableDependencies.size() == 0) {
                        logger.info("No Vulnerabilities Found. Skipping processing of revision.");
                        persistOutput(revision, "{}");
                        continue;
                    } else {
                        logger.info("Vulnerable IDs: " + vulnerableDependencies.toString());
                    }

                    depIds.add(revision.id);
                    var databaseMerger = new CGMerger(depIds, dbContext, graphDao);
                    var graph = databaseMerger.mergeAllDeps();
                    if (graph == null || graph.numNodes() == 0) {
                        logger.info("Empty Graph for: " + revision.toString());
                        persistOutput(revision, "{}");
                        continue;
                    } else {
                        logger.info("Graph exists for: " + revision.toString());
                    }

                    var vulnerabilities = kbDao.findVulnerableCallables(vulnerableDependencies, graph.nodes());
                    logger.info("Vulnerable Callables: " + vulnerabilities.keySet().size());

                    var internalCallables = kbDao.getPackageInternalCallableIDs(revision.product().toString(), revision.version.toString());
                    logger.info("Internal Callables: " + internalCallables.size());

                    logger.info("Quick graph validation...");
                    var a = 0;
                    for (var vuln : vulnerabilities.keySet()) {
                        if (graph.containsVertex(vuln)) a++;
                    }
                    logger.info("Graph contains vulnerable callables: " + a + "/" + vulnerabilities.keySet().size());
                    var b = 0;
                    for (var vuln : internalCallables) {
                        if (graph.containsVertex(vuln)) b++;
                    }
                    logger.info("Graph contains internal callable: " + b + "/" + internalCallables.size());


                    // Find all paths between any internal node and any vulnerable node in the graph
//                    var vulnerablePaths = new ArrayList<List<Long>>();
//                    for (var internal : internalCallables) {
//                        for (var vulnerable : vulnerabilities.keySet()) {
//                            vulnerablePaths.add(getPathsToVulnerableNode(graph, internal, vulnerable, new HashSet<>(), new ArrayList<>()));
//                        }
//                    }
//                    logger.info("Vulnerable Paths: " + vulnerablePaths.toString());

                    var l = new ArrayList<Long>(2);
                    l.add(internalCallables.get(0));
                    l.add(kbDao.getPackageVersionCallable((Long) vulnerableDependencies.toArray()[0]));

                    var vulnerablePaths = new ArrayList<List<Long>>(1);
                    vulnerablePaths.add(l);

                    logger.info("Vulnerable Paths: " + vulnerablePaths.toString());

                    var pathNodes = new HashSet<Long>();
                    vulnerablePaths.forEach(pathNodes::addAll);
                    var fastenUris = kbDao.getFullFastenUris(new ArrayList<>(pathNodes));

                    // Generate JSON response
                    var output = new JSONObject();
                    var pathsJson = new JSONArray();
                    for (var entry : vulnerablePaths) {
                        var pathJson = new JSONArray();
                        for (var node : entry) {
                            var fullUriParsed = FastenUriUtils.parseFullFastenUri(fastenUris.get(node));
                            var partialUriParsed = FastenUriUtils.parsePartialFastenUri(fullUriParsed.get(3));

                            var jsonNode = new JSONObject();
                            jsonNode.put("id", node);
                            jsonNode.put("package_name", fullUriParsed.get(1));
                            jsonNode.put("version", fullUriParsed.get(2));
                            jsonNode.put("class_name", partialUriParsed.get(1));
                            jsonNode.put("method_name", partialUriParsed.get(2));
                            pathJson.put(jsonNode);
                        }
                        pathsJson.put(pathJson);
                    }
                    output.put("vulnerabilities", pathsJson);
                    persistOutput(revision, output.toString());
                }
            } catch (JSONException e) {
                logger.error("Failed to parse json payload", e);
                setPluginError(new RuntimeException("Failed to parse json payload", e));
            } catch (Exception e) {
                logger.error("Arbitrary exception raised", e);
                setPluginError(new RuntimeException("Arbitrary exception raised", e));
            }

        }

        @Override
        public Optional<String> produce() {
            return Optional.empty();
        }

        @Override
        public String getOutputPath() {
            return null;
        }

    }
}

